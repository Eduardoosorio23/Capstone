{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas_datareader as web\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "from keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from  nltk import FreqDist\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.collocations import *\n",
    "# nltk.download('wordnet')\n",
    "from nltk import word_tokenize, FreqDist\n",
    "\n",
    "from spacy import displacy\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.add_pipe('spacytextblob')\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Webscraped data from Reddit and twitter talking about TSLA stock\n",
    "df = pd.read_csv('static_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data=> (10193, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>post</th>\n",
       "      <th>date</th>\n",
       "      <th>spacy</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>$BBBY DD</td>\n",
       "      <td>2021-06-02</td>\n",
       "      <td>$BBBY DD</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Here comes the sun turururu $RUN $TSLA</td>\n",
       "      <td>2021-06-02</td>\n",
       "      <td>Here comes the sun turururu $RUN $TSLA</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>$Run $Tsla f*** the suits</td>\n",
       "      <td>2021-06-02</td>\n",
       "      <td>$Run $Tsla f*** the suits</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>B.A.R.S - The Ultimate Meme Portfolio ðŸš€ðŸš€ðŸš€</td>\n",
       "      <td>2021-06-01</td>\n",
       "      <td>B.A.R.S - The Ultimate Meme Portfolio ðŸš€ðŸš€ðŸš€</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>(TSLA) vs (F)</td>\n",
       "      <td>2021-06-01</td>\n",
       "      <td>(TSLA) vs (F)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                       post        date  \\\n",
       "0           0                                   $BBBY DD  2021-06-02   \n",
       "1           1     Here comes the sun turururu $RUN $TSLA  2021-06-02   \n",
       "2           2                  $Run $Tsla f*** the suits  2021-06-02   \n",
       "3           3  B.A.R.S - The Ultimate Meme Portfolio ðŸš€ðŸš€ðŸš€  2021-06-01   \n",
       "4           4                              (TSLA) vs (F)  2021-06-01   \n",
       "\n",
       "                                       spacy  sentiment  subjectivity  \n",
       "0                                   $BBBY DD        0.0           0.0  \n",
       "1     Here comes the sun turururu $RUN $TSLA        0.0           0.0  \n",
       "2                  $Run $Tsla f*** the suits        0.0           0.0  \n",
       "3  B.A.R.S - The Ultimate Meme Portfolio ðŸš€ðŸš€ðŸš€        0.0           1.0  \n",
       "4                              (TSLA) vs (F)        0.0           0.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Shape of the Dataset\n",
    "print(\"Shape of data=>\",df.shape)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0      0\n",
       "post            0\n",
       "date            0\n",
       "spacy           0\n",
       "sentiment       0\n",
       "subjectivity    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking for Nulls\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posts 1:\n",
      " $SOS clean and green till Elon gets a ring. WE SHALL RISE\n",
      "Posts 2:\n",
      " Is $SPCE the next $GME.\n",
      "Posts 3:\n",
      " F IS SUPER UNDERVALUED\n",
      "Posts 4:\n",
      " $GOGO is a Starlink play\n",
      "Posts 5:\n",
      " WOW! Look what is happening with Ford (F)! It have reached 15 years high and not stoping, I think it will be one of those stocks that get tremendous momentum! Let is watch!\n"
     ]
    }
   ],
   "source": [
    "# taking a look at 5 random posts\n",
    "for index,text in enumerate(df['post'][35:40]):\n",
    "  print('Posts %d:\\n'%(index+1),text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of English Contractions\n",
    "contractions_dict = { \"ain't\": \"are not\",\"'s\":\" is\",\"aren't\": \"are not\",\n",
    "                     \"can't\": \"cannot\",\"can't've\": \"cannot have\",\n",
    "                     \"'cause\": \"because\",\"could've\": \"could have\",\"couldn't\": \"could not\",\n",
    "                     \"couldn't've\": \"could not have\", \"didn't\": \"did not\",\"doesn't\": \"does not\",\n",
    "                     \"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\n",
    "                     \"hasn't\": \"has not\",\"haven't\": \"have not\",\"he'd\": \"he would\",\n",
    "                     \"he'd've\": \"he would have\",\"he'll\": \"he will\", \"he'll've\": \"he will have\",\n",
    "                     \"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\n",
    "                     \"I'd\": \"I would\", \"I'd've\": \"I would have\",\"I'll\": \"I will\",\n",
    "                     \"I'll've\": \"I will have\",\"I'm\": \"I am\",\"I've\": \"I have\", \"isn't\": \"is not\",\n",
    "                     \"it'd\": \"it would\",\"it'd've\": \"it would have\",\"it'll\": \"it will\",\n",
    "                     \"it'll've\": \"it will have\", \"let's\": \"let us\",\"ma'am\": \"madam\",\n",
    "                     \"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\", \n",
    "                     \"mightn't've\": \"might not have\",\"must've\": \"must have\",\"mustn't\": \"must not\",\n",
    "                     \"mustn't've\": \"must not have\", \"needn't\": \"need not\",\n",
    "                     \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\n",
    "                     \"oughtn't've\": \"ought not have\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\n",
    "                     \"shan't've\": \"shall not have\",\"she'd\": \"she would\",\"she'd've\": \"she would have\",\n",
    "                     \"she'll\": \"she will\", \"she'll've\": \"she will have\",\"should've\": \"should have\",\n",
    "                     \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\"so've\": \"so have\",\n",
    "                     \"that'd\": \"that would\",\"that'd've\": \"that would have\", \"there'd\": \"there would\",\n",
    "                     \"there'd've\": \"there would have\", \"they'd\": \"they would\",\n",
    "                     \"they'd've\": \"they would have\",\"they'll\": \"they will\",\n",
    "                     \"they'll've\": \"they will have\", \"they're\": \"they are\",\"they've\": \"they have\",\n",
    "                     \"to've\": \"to have\",\"wasn't\": \"was not\",\"we'd\": \"we would\",\n",
    "                     \"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\n",
    "                     \"we're\": \"we are\",\"we've\": \"we have\", \"weren't\": \"were not\",\"what'll\": \"what will\",\n",
    "                     \"what'll've\": \"what will have\",\"what're\": \"what are\", \"what've\": \"what have\",\n",
    "                     \"when've\": \"when have\",\"where'd\": \"where did\", \"where've\": \"where have\",\n",
    "                     \"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who've\": \"who have\",\n",
    "                     \"why've\": \"why have\",\"will've\": \"will have\",\"won't\": \"will not\",\n",
    "                     \"won't've\": \"will not have\", \"would've\": \"would have\",\"wouldn't\": \"would not\",\n",
    "                     \"wouldn't've\": \"would not have\",\"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
    "                     \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n",
    "                     \"y'all've\": \"you all have\", \"you'd\": \"you would\",\"you'd've\": \"you would have\",\n",
    "                     \"you'll\": \"you will\",\"you'll've\": \"you will have\", \"you're\": \"you are\",\n",
    "                     \"you've\": \"you have\"}\n",
    "\n",
    "# Regular expression for finding contractions\n",
    "contractions_re=re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
    "\n",
    "# Function for expanding contractions\n",
    "def expand_contractions(text,contractions_dict=contractions_dict):\n",
    "  def replace(match):\n",
    "    return contractions_dict[match.group(0)]\n",
    "  return contractions_re.sub(replace, text)\n",
    "\n",
    "# Expanding Contractions in the reviews\n",
    "df['post']=df['post'].apply(lambda x:expand_contractions(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posts 1:\n",
      " $SOS clean and green till Elon gets a ring. WE SHALL RISE\n",
      "Posts 2:\n",
      " Is $SPCE the next $GME.\n",
      "Posts 3:\n",
      " F IS SUPER UNDERVALUED\n",
      "Posts 4:\n",
      " $GOGO is a Starlink play\n",
      "Posts 5:\n",
      " WOW! Look what is happening with Ford (F)! It have reached 15 years high and not stoping, I think it will be one of those stocks that get tremendous momentum! Let is watch!\n"
     ]
    }
   ],
   "source": [
    "# taking a look at 5 posts after expanding contrations\n",
    "for index,text in enumerate(df['post'][35:40]):\n",
    "  print('Posts %d:\\n'%(index+1),text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the posts into lower case for the purposes of NLP\n",
    "df['post']=df['post'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing numbers and words containing numbers\n",
    "df['post']=df['post'].apply(lambda x: re.sub('\\w*\\d\\w*','', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing punctuation\n",
    "df['post']=df['post'].apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation), '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing extra spaces\n",
    "df['post']=df['post'].apply(lambda x: re.sub(' +',' ',x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posts 1:\n",
      " bbby dd\n",
      "Posts 2:\n",
      " here comes the sun turururu run tsla\n",
      "Posts 3:\n",
      " run tsla f the suits\n",
      "Posts 4:\n",
      " bars the ultimate meme portfolio ðŸš€ðŸš€ðŸš€\n",
      "Posts 5:\n",
      " tsla vs f\n"
     ]
    }
   ],
   "source": [
    "# taking a look at 5 random posts\n",
    "for index,text in enumerate(df['post'][0:5]):\n",
    "  print('Posts %d:\\n'%(index+1),text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatizing the tokens\n",
    "df['lemmatized']=df['post'].apply(lambda x: ' '.join([token.lemma_ for token in list(nlp(x)) if (token.is_stop==False)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>post</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>\u0010nio will be worthless in a few years 1 3</th>\n",
       "      <td>\u0010nio worthless year 1 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avatar le dernier maÃ®tre de lâ€™air la sÃ©rie en liveaction cowboy bebop la sÃ©rie en liveaction one piece la sÃ©rie en liveaction cyberpunk edgerunners les sÃ©ries et films narnia la sÃ©rie dâ€™animation splinter cell le documentaire sur lâ€™affaire gamestop</th>\n",
       "      <td>avatar le dernier maÃ®tre de lâ€™air la sÃ©rie e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hereâ€™s some quick tsla dd before earnings ib has an amazing mobile app</th>\n",
       "      <td>quick tsla dd earning ib amazing mobile app</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hwang seeds ark ark buys tsla mysterious otm call buys in tsla gamma squeeze the stock by 10x higher for 18 months hwang buys gsx mysterious otm call buys in gsx gamma squeeze shorts and 4x the stock hwang blows up after gamma squeezes unwind what is next for ark</th>\n",
       "      <td>hwang seed ark ark buy tsla mysterious otm b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                           lemmatized\n",
       "post                                                                                                 \n",
       "                                                                                                     \n",
       "\u0010nio will be worthless in a few years 1 3                                     \u0010nio worthless year 1 3\n",
       " avatar le dernier maÃ®tre de lâ€™air la sÃ©rie en ...    avatar le dernier maÃ®tre de lâ€™air la sÃ©rie e...\n",
       " hereâ€™s some quick tsla dd before earnings ib h...        quick tsla dd earning ib amazing mobile app\n",
       " hwang seeds ark ark buys tsla mysterious otm c...    hwang seed ark ark buy tsla mysterious otm b..."
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_grouped=df[['post','lemmatized']].groupby(by='post').agg(lambda x:' '.join(x))\n",
    "df_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:learn-env] *",
   "language": "python",
   "name": "conda-env-learn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
